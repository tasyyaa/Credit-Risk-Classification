# -*- coding: utf-8 -*-
"""Proyek Pertama: Predictive Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w-qnUIcRjTmxDjPX_ZWVaElVyIaO1ElC

# Credit Risk Customer Classification Project

##Import Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler
from sklearn.preprocessing import StandardScaler,  OneHotEncoder
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.model_selection import KFold, StratifiedKFold, RepeatedKFold, RepeatedStratifiedKFold, ShuffleSplit, StratifiedShuffleSplit
from sklearn.pipeline import Pipeline
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.impute import SimpleImputer
from sklearn.compose import make_column_transformer
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import f1_score
from sklearn.pipeline import make_pipeline
from scipy import stats
from sklearn.decomposition import PCA
from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier
from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import LinearSVC
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import cross_val_score

"""##Load Dataset"""

!gdown --id "19olO2TEhXwNs0N_O6HQ162kOgW9lGBRT"

!unzip "Credit Risk Dataset.zip" -d "/content/Credit Risk Dataset"

credit = pd.read_csv("/content/Credit Risk Dataset/credit_customers.csv")
credit.head(10)

"""##Data Understanding

Dilakukan pemahaman data dengan melakukan beberapa tahap yaitu data description dan EDA

###Data Description

####Mengubah Data Object

Melakukan pengecekan tipe data dan banyak variabel yang dimiliki
"""

credit.info()

"""Mengubah tipe data agar sesuai"""

#Mengubah Tipe Data
credit['checking_status'] = credit['checking_status'].astype('category')
credit['credit_history'] = credit['credit_history'].astype('category')
credit['purpose'] = credit['purpose'].astype('category')
credit['savings_status'] = credit['savings_status'].astype('category')
credit['employment'] = credit['employment'].astype('category')
credit['personal_status'] = credit['personal_status'].astype('category')
credit['other_parties'] = credit['other_parties'].astype('category')
credit['property_magnitude'] = credit['property_magnitude'].astype('category')
credit['other_payment_plans'] = credit['other_payment_plans'].astype('category')
credit['housing'] = credit['housing'].astype('category')
credit['job'] = credit['job'].astype('category')
credit['own_telephone'] = credit['own_telephone'].astype('category')
credit['foreign_worker'] = credit['foreign_worker'].astype('category')
credit['class'] = credit['class'].astype('category')
credit.info()

"""####Deskripsi Persebaran Data

Melihat persebaran data setiap variabel
"""

credit.describe()

"""###Explanatory Data Analysis (EDA)

Melakukan ekplorasi untuk mengetahui karakteristik data

####Univariate Analysis

Analisis masing masing variabel

#####Bar Chart for Categorical data features

Visualisasi untuk data berkategori barchart
"""

for col in credit.select_dtypes(include='category').columns:
    plt.figure(figsize=(12, 6))  # Mengatur ukuran figure
    sns.countplot(data=credit, x=credit[col], palette='Set1', width=0.6)  # Mengatur lebar bar
    plt.title(f"Bar Chart of {col}")
    plt.xlabel(col)
    plt.ylabel('Count')
    plt.show()

"""#####Histogram untuk Numerical data features

Visualisasi untuk data numerical menggunakan histogram dan boxplot
"""

credit.hist(bins=50, figsize=(20,15))
plt.show()

"""#####Boxplot for Numerical Data Features"""

for col in credit.select_dtypes(include="number").columns:
  plt.figure(figsize = (5, 5))
  credit[col].plot(kind="box")
  plt.show()

"""####Multivariate Analysis

Analisis variabel satu dengan variabel lain yang bisa juga menggunakan variabel target class

#####Categorical Values

Visualisasi dengan categorical values menggunakan bar chart
"""

cat_features = credit.select_dtypes(include='category').columns.to_list()

for col in cat_features:
  sns.catplot(x=col, y="class", kind="bar", dodge=False, height =4, aspect =3, data=credit, palette="Set3")
  plt.title("Rata-rata 'class' Relatif terhadap - {}".format(col))

"""#####Numerical Values

Membuat visualisasi numerical menggunakan scatter plot
"""

numerical_features = credit.select_dtypes(include='number').columns.to_list()

sns.pairplot(data=credit, hue='class')

"""###Correlation

Melihat hubungan antarvariabel
"""

plt.figure(figsize=(10, 8))
correlation_matrix = credit[numerical_features].corr().round(2)

# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""##Data Preparation

Membersihkan data dan mempersiapkan data agar dapat digunakan dalam modeling

####Memeriksa Missing Value

Melakukan pemeriksaan dari data yang hilang
"""

# Cek apakah terdapat baris yang kosong
credit.isnull().sum()

col_with_unknown = []
unknown_count = (credit == "unknown").sum()
for col, count in unknown_count.items():
    value_count = credit[col].value_counts()
    if count > 0:
        col_with_unknown.append(col)
        print(f"{col} {value_count} \n")

"""Tidak ada data yang hilang

####Memeriksa data duplikat

Melakukan pemeriksaan data duplikat
"""

#Cek data yang duplikat
jumlah_baris_duplikat = credit.duplicated().sum()
print("Jumlah baris yang memiliki data sama: {} baris".format(jumlah_baris_duplikat))

"""Tidak ada data duplikat

####Outlier Handling

Mencari outlier dan menghapusnya

Gambaran outlier
"""

for col in credit.select_dtypes(include="number").columns:
  plt.figure(figsize = (5, 5))
  credit[col].plot(kind="box")
  plt.show()

"""Menghitung outlier menggunakan Z_score dan menghapus"""

# threshold value for z-score
threshold = 3

# Menghapus outlier menggunakan Z-score dengan Threshold 3
def remove_outliers_zscore_loop(df, threshold=3):
    credit_process = df.copy()  # Pastikan menggunakan DataFrame yang benar
    outliers = np.ones(len(credit_process), dtype=bool)
    for col in credit_process.select_dtypes(include=np.number).columns:
        z_scores = np.abs((credit_process[col] - credit_process[col].mean()) / credit_process[col].std())
        outliers = outliers & (z_scores <= threshold)  # Gabungkan mask boolean dengan AND
    credit_no_outliers = credit_process[outliers]
    return credit_no_outliers

credit_cleaned = remove_outliers_zscore_loop(credit)
print(credit_cleaned)

"""tersisa 952 data yang terbebas dari outlier

####Melakukan categorical encoding

Mengubah data categorical menjadi numerical dengan label encoding
"""

credit_cleaned.info()

"""Data categorical di bawah ini dijadikan bilang 1 dan 0"""

credit_encoded = credit_cleaned.replace({'own_telephone': {'yes': 1, 'none': 0}})
credit_encoded = credit_encoded.replace({'foreign_worker': {'yes': 1, 'no': 0}})
credit_encoded = credit_encoded.replace({'class': {'good': 1, 'bad': 0}})

"""melakukan label encoding pada variabel categorical lainnya"""

# Mencari kategori apa saja pada sebuah kolom
kategori_checking_status = credit_cleaned['checking_status'].unique()
kategori_credit_history = credit_cleaned['credit_history'].unique()
kategori_purpose = credit_cleaned['purpose'].unique()
kategori_savings_status = credit_cleaned['savings_status'].unique()
kategori_employment = credit_cleaned['employment'].unique()
kategori_personal_status = credit_cleaned['personal_status'].unique()
kategori_other_parties = credit_cleaned['other_parties'].unique()
kategori_property_magnitude = credit_cleaned['property_magnitude'].unique()
kategori_other_payment_plans = credit_cleaned['other_payment_plans'].unique()
kategori_housing = credit_cleaned['housing'].unique()
kategori_job = credit_cleaned['job'].unique()

print('kategori checking status:',kategori_checking_status)
print('kategori credit history:',kategori_credit_history)
print('kategori purpose:',kategori_purpose)
print('kategori savings status:',kategori_savings_status)
print('kategori employment:',kategori_employment)
print('kategori personal status:',kategori_personal_status)
print('kategori other parties:',kategori_other_parties)
print('kategori property magnitude:',kategori_property_magnitude)
print('kategori other payment plans:',kategori_other_payment_plans)
print('kategori housing:',kategori_housing)
print('kategori job:',kategori_job)

for col in credit.select_dtypes(include='category').columns:
  credit_encoded[col] = credit_encoded[col].cat.codes
credit_encoded

"""####Visualisasi setelah pembersihan data"""

for col in credit_encoded.select_dtypes(include="number").columns:
  plt.figure(figsize = (5, 5))
  credit_encoded[col].plot(kind="box")
  plt.show()

plt.figure(figsize = (10, 10))
credit_encoded.hist()
plt.subplots_adjust(hspace=0.5)
plt.show()

"""####Korelasi Setelah pembersihan data

Berikut merupakan korelasi setelah pembersihan data
"""

credit_encoded= credit_encoded.astype('int64')

credit_encoded.info()

credit_encoded.corrwith(credit_encoded["class"]).sort_values(ascending=False)

correlation_matrix = credit_encoded.corr().round(2)

# Plotting heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title("Correlation Matrix untuk Semua Fitur", size=20)
plt.show()

# Menampilkan matriks korelasi
print(correlation_matrix)

"""Berdasarkan hasil tidak ada variabel yang memiliki korelasi kuat baik positif atau negatif namun terdapat yang lebih kuat dari yang lainnya, yaitu untuk positif adalah checking_status, purpose, dan age. Sedangkan untik negatif adalah credit_history, foreign worker, installment commitment, credit_amount, dan duration.

####PCA

Dilakukan untuk mereduksi dimensi
"""

sns.pairplot(credit_encoded[['checking_status','purpose','age']], plot_kws={"s": 3});

pca = PCA(n_components=3, random_state=123)
pca.fit(credit_encoded[['checking_status','purpose','age']])
princ_comp = pca.transform(credit_encoded[['checking_status','purpose','age']])

pca.explained_variance_ratio_.round(3)

"""Menggabungkan tiga variabel menjadi satu saja yang menggambarkan ketiga agar dimensi mengecil"""

pca=PCA(n_components=1, random_state=123)
pca.fit(credit_encoded[['checking_status','purpose','age']])
credit_encoded['PCA'] = pca.transform(credit_encoded.loc[:,('checking_status','purpose','age')]).flatten()
credit_encoded.drop(['checking_status','purpose','age'], axis=1, inplace=True)

credit_encoded.head()

"""####Data Balancing

Penyeimbangan data variabel class karena condong ke arah risiko kredit baik
"""

credit_encoded['class'].value_counts()

X = credit_encoded.drop('class', axis=1)
y = credit_encoded['class']

balancer = RandomOverSampler(random_state=42)
X_balanced, y_balanced = balancer.fit_resample(X, y)
y_balanced.value_counts()

"""####Data Splitting

Pembagian data menjadi dataset training dan dataset testing
"""

from sklearn.model_selection import train_test_split

X = X_balanced
y = y_balanced
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset:{len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""####Feature Scaling

Melakukan standarisasi atau normalisasi agar persebaran data tidak berbeda antar satu variabel dengan variabel lainnya
"""

X.describe()

scaler = StandardScaler()
X_train['duration'] = scaler.fit_transform(X_train[['duration']]) # "fit" on the TRAIN set only, then transform
X_test['duration'] = scaler.transform(X_test[['duration']]) # while on the TEST set, just "transform" it

X_train['credit_amount'] = scaler.fit_transform(X_train[['credit_amount']])
X_test['credit_amount'] = scaler.transform(X_test[['credit_amount']])

X_train['PCA'] = scaler.fit_transform(X_train[['PCA']])
X_test['PCA'] = scaler.transform(X_test[['PCA']])


X_train.describe()

"""##Model Development

###Decision Tree
"""

# Membuat model Decision Tree
dtree = DecisionTreeClassifier()

# Membuat grid parameter yang akan diuji
param_grid = {
    'max_depth': [None, 10, 20, 30, 40, 50],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Membuat objek GridSearchCV
grid_search = GridSearchCV(dtree, param_grid, cv=5, scoring='accuracy')

# Melakukan pencarian parameter terbaik pada data latih
grid_search.fit(X_train, y_train)

# Menampilkan parameter terbaik
print("Parameter terbaik:", grid_search.best_params_)

# Inisialisasi model Decision Tree dengan parameter yang disebutkan
dtree = DecisionTreeClassifier(max_depth=None, min_samples_leaf=1, min_samples_split=2)
dtree.fit(X_train, y_train)
y_pred = dtree.predict(X_test)
f1_score_dtree = f1_score(y_test, y_pred)
roc_auc_dtree = roc_auc_score(y_test, y_pred)
accuracy_dtree = accuracy_score(y_test, y_pred)
print(f"F1-score: {f1_score_dtree}")
print(f"ROC-AUC Score: {roc_auc_dtree}\n")
print('Accuracy:', accuracy_dtree)
print('Classification report:\n', metrics.classification_report(y_test, y_pred))

"""###K-Nearest Neighbor"""

knn = KNeighborsClassifier()
param_grid = {
    'n_neighbors': [1, 3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'p': [1, 2],
}
grid_search = GridSearchCV(knn, param_grid, cv=5)
grid_search.fit(X, y)
print("Parameter terbaik:", grid_search.best_params_)
print("Skor validasi terbaik:", grid_search.best_score_)

knn = KNeighborsClassifier(n_neighbors=1, p=2, weights="uniform")
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
f1_score_knn = f1_score(y_test, y_pred)
roc_auc_knn = roc_auc_score(y_test, y_pred)
accuracy_knn = accuracy_score(y_test, y_pred)
print(f"F1-score: {f1_score_knn}")
print(f"ROC-AUC Score: {roc_auc_knn}\n")
print('Accuracy:', accuracy_knn)
print('Classification report:\n', metrics.classification_report(y_test, y_pred))

"""###Backpropagation Neural Network"""

nn = MLPClassifier(activation="relu", solver="sgd", alpha=0.001)
nn.fit(X_train, y_train)
y_pred = nn.predict(X_test)
f1_score_nn = f1_score(y_test, y_pred)
roc_auc_nn = roc_auc_score(y_test, y_pred)
accuracy_nn = accuracy_score(y_test, y_pred)
print(f"F1-score: {f1_score_nn}")
print(f"ROC-AUC Score: {roc_auc_nn}\n")
print('Accuracy:', accuracy_nn)
print('Classification report:\n', metrics.classification_report(y_test, y_pred))

"""###Random Forest"""

# Tentukan model Random Forest
rf = RandomForestClassifier()

# Tentukan parameter yang akan diuji
param_grid = {
    'n_estimators': [100, 200, 300, 400, 500],
    'max_depth': [None, 10, 20, 30, 40],
    'min_samples_split': [2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8]
}

# Gunakan  Randomized Search untuk mencari parameter terbaik
random_search = RandomizedSearchCV(rf, param_distributions=param_grid, n_iter=10, cv=5, scoring='accuracy', random_state=123, n_jobs=-1)

# Latih Data
random_search.fit(X_train, y_train)

#Cetak Parameter terbaik
best_params = random_search.best_params_
print("Parameter Terbaik:", best_params)

# Model dengan parameter terbaik
best_model = random_search.best_estimator_

rf = RandomForestClassifier(n_estimators= 300, min_samples_split= 2, min_samples_leaf= 2, max_depth= 40)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
f1_score_rf = f1_score(y_test, y_pred)
roc_auc_rf = roc_auc_score(y_test, y_pred)
accuracy_rf = accuracy_score(y_test, y_pred)
print(f"F1-score: {f1_score_rf}")
print(f"ROC-AUC Score: {roc_auc_rf}\n")
print('Accuracy:', accuracy_rf)
print('Classification report:\n', metrics.classification_report(y_test, y_pred))

"""###Logistic Regression"""

param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Variasi parameter C
    'solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga', 'newton-cholesky' ]  # Variasi solver
}

# Membuat objek model Regresi Logistik
logreg = LogisticRegression(random_state=123)

# Membuat objek GridSearchCV
grid_search = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1)

# Melatih model dengan parameter terbaik
grid_search.fit(X_train, y_train)

# Menampilkan parameter terbaik
print("Parameter terbaik:", grid_search.best_params_)

logreg = LogisticRegression(C=1, solver='liblinear', random_state=123)
logreg.fit(X_train, y_train)
y_pred = logreg.predict(X_test)
f1_score_logreg = f1_score(y_test, y_pred)
roc_auc_logreg = roc_auc_score(y_test, y_pred)
accuracy_logreg = accuracy_score(y_test, y_pred)
print(f"F1-score: {f1_score_logreg}")
print(f"ROC-AUC Score: {roc_auc_logreg}\n")
print('Accuracy:', accuracy_logreg)
print('Classification report:\n', metrics.classification_report(y_test, y_pred))

"""##Evaluasi Model"""

metode = ["Decision Tree", "K-Nearest Neighbor",  "Neural Networks", "Random Forest", "Logistic Regression"]
f1_score_test = [f1_score_dtree, f1_score_knn, f1_score_nn,  f1_score_rf, f1_score_logreg]
roc_auc_score_test = [roc_auc_dtree, roc_auc_knn,  roc_auc_nn, roc_auc_rf, roc_auc_logreg]
accuracy_score_test = [accuracy_dtree, accuracy_knn,  accuracy_nn, accuracy_rf, accuracy_logreg]

# Membuat DataFrame
data = {
    "Metode": metode,
    "F1-Score Test": f1_score_test,
    "ROC-AUC Score Test": roc_auc_score_test,
    "Accuracy Score Test": accuracy_score_test,

}

Model_selection = pd.DataFrame(data)
Model_selection

"""Berdasarkan hasil didapatkan bahwa Random Forest memberikan hasil terbaik untuk seluruh metrik evaluasi yaitu 0.567"""